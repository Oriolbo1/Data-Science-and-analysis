{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Analytics Case Study (Databricks Notebook) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will carry out the module exercises. \n",
    "\n",
    "The solution must be coded and executed in the cells just below the statements of the exercises.\n",
    "\n",
    "Once it is finished, you can export the notebook in DBC (Databricks Notebook) format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">2.4.5\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises will consist of adding new functionalities, or executing new code, on the Notebook that contains all the theory seen in the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[4]: True</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"/FileStore/tables/Hotel_Reviews.csv\", \"file:///databricks/driver/Hotel_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def score_to_string(score):\n",
    "  if score < 5:\n",
    "    return \"Bad\"\n",
    "  elif score < 7:\n",
    "    return \"Normal\"\n",
    "  elif score < 9:\n",
    "    return \"Good\"\n",
    "  elif score < 10: \n",
    "    return \"Excellent\"\n",
    "  else:\n",
    "    return \"Perfect\"\n",
    "  \n",
    "def score_to_evaluation(score_string):\n",
    "  score_dict = {\n",
    "    \"Bad\": 0,\n",
    "    \"Normal\": 1,\n",
    "    \"Good\": 2,\n",
    "    \"Excellent\": 3,\n",
    "    \"Perfect\": 4\n",
    "  }\n",
    "  return score_dict.get(score_string, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames in Spark: SparkSQL. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_sql = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\", \"true\")\\\n",
    "         .option(\"inferSchema\", \"true\")\\\n",
    "         .load(\"/FileStore/tables/Hotel_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "score_string_udf = udf(score_to_string, StringType())\n",
    "score_evaluation_udf = udf(score_to_evaluation, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_sql = df_spark_sql.withColumn('score_string',score_string_udf(df_spark_sql[\"Average_Score\"]))\n",
    "df_spark_sql = df_spark_sql.withColumn('score_evaluation',score_evaluation_udf(df_spark_sql[\"score_string\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def day_to_int(day):\n",
    "  return int(day.replace(\" days\", \"\").replace(\" day\", \"\"))\n",
    "day_to_int_udf = udf(day_to_int, IntegerType())\n",
    "df_spark_sql = df_spark_sql.withColumn(\"days_since_review\", day_to_int_udf(df_spark_sql[\"days_since_review\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create a loop that shows all the columns of the DataFrame, along with their types. You can also paint the outline of the Dataframe. ###\n",
    "\n",
    "I'm going to start by creating a loop with for \"i\" in df_spark_sql.dtypes and printing \"i\" on each iteration. In this way it prints the information it contains, in this case the name of the column and its type.\n",
    "\n",
    "Next I am going to use the code \"df_spark_sql.printSchema ()\" to get the schema of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">----tipos de datos----\n",
       "(&#39;Hotel_Address&#39;, &#39;string&#39;)\n",
       "(&#39;Additional_Number_of_Scoring&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Date&#39;, &#39;string&#39;)\n",
       "(&#39;Average_Score&#39;, &#39;double&#39;)\n",
       "(&#39;Hotel_Name&#39;, &#39;string&#39;)\n",
       "(&#39;Reviewer_Nationality&#39;, &#39;string&#39;)\n",
       "(&#39;Negative_Review&#39;, &#39;string&#39;)\n",
       "(&#39;Review_Total_Negative_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews&#39;, &#39;int&#39;)\n",
       "(&#39;Positive_Review&#39;, &#39;string&#39;)\n",
       "(&#39;Review_Total_Positive_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews_Reviewer_Has_Given&#39;, &#39;int&#39;)\n",
       "(&#39;Reviewer_Score&#39;, &#39;double&#39;)\n",
       "(&#39;Tags&#39;, &#39;string&#39;)\n",
       "(&#39;days_since_review&#39;, &#39;int&#39;)\n",
       "(&#39;lat&#39;, &#39;float&#39;)\n",
       "(&#39;lng&#39;, &#39;float&#39;)\n",
       "(&#39;score_string&#39;, &#39;string&#39;)\n",
       "(&#39;score_evaluation&#39;, &#39;int&#39;)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "----Esquema----\n",
       "root\n",
       "-- Hotel_Address: string (nullable = true)\n",
       "-- Additional_Number_of_Scoring: integer (nullable = true)\n",
       "-- Review_Date: string (nullable = true)\n",
       "-- Average_Score: double (nullable = true)\n",
       "-- Hotel_Name: string (nullable = true)\n",
       "-- Reviewer_Nationality: string (nullable = true)\n",
       "-- Negative_Review: string (nullable = true)\n",
       "-- Review_Total_Negative_Word_Counts: integer (nullable = true)\n",
       "-- Total_Number_of_Reviews: integer (nullable = true)\n",
       "-- Positive_Review: string (nullable = true)\n",
       "-- Review_Total_Positive_Word_Counts: integer (nullable = true)\n",
       "-- Total_Number_of_Reviews_Reviewer_Has_Given: integer (nullable = true)\n",
       "-- Reviewer_Score: double (nullable = true)\n",
       "-- Tags: string (nullable = true)\n",
       "-- days_since_review: integer (nullable = true)\n",
       "-- lat: float (nullable = true)\n",
       "-- lng: float (nullable = true)\n",
       "-- score_string: string (nullable = true)\n",
       "-- score_evaluation: integer (nullable = true)\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"----tipos de datos----\")\n",
    "for i in df_spark_sql.dtypes:\n",
    "  print(i)\n",
    "  \n",
    "print(\"\\n\"*3)\n",
    "print(\"----Esquema----\")\n",
    "df_spark_sql.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sample 10 unique hotel name values. Sort them alphanumerically in ascending order (numbers 0-9 first, then A-Z).###\n",
    "\n",
    "I start by selecting the column 'Hotel_Name'. Next I tell the code to only select the unique values. Later I limit the number of results to 10 and finally I order the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+\n",
       "          Hotel_Name|\n",
       "+--------------------+\n",
       "      Avenida Palace|\n",
       "Best Western Sera...|\n",
       "Grange Holborn Hotel|\n",
       "H tel Barri re Le...|\n",
       "H tel Elysees Mermoz|\n",
       " H10 Port Vell 4 Sup|\n",
       "         HCC Regente|\n",
       "Hyatt Regency Ams...|\n",
       "Melia Paris Notre...|\n",
       "Melia Paris Tour ...|\n",
       "+--------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_sql.select('Hotel_Name').distinct().limit(10).orderBy('Hotel_Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Transform the * lat * and * lng * columns to the Float type.\n",
    "\n",
    "I start by converting the \"lat\" column. For this purpose I use the \"withColumn\" function that creates a new column, however, since the name of the \"new\" column is the same as an existing column in the dataframe, the function used will replace the column that was in the dataframe.\n",
    "\n",
    "Then I make reference to the column 'df_spark_sql [\"lat\"]' and finish this part by indicating that I want it to become a \"Float\" type.\n",
    "\n",
    "I apply the same procedure with the feature \"lng\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_sql = df_spark_sql.withColumn(\"lat\", df_spark_sql[\"lat\"].cast(\"Float\"))\n",
    "df_spark_sql = df_spark_sql.withColumn(\"lng\", df_spark_sql[\"lng\"].cast(\"Float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">342967\n",
       "169503\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits = df_spark_sql.randomSplit([0.67, 0.33])\n",
    "df_spark_sql_train = splits[0].dropna()\n",
    "df_spark_sql_test = splits[1].dropna()\n",
    "print(df_spark_sql_train.count())\n",
    "print(df_spark_sql_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: How many hotels have a 'Perfect' score? And 'Good'? And 'Normal' next to 'Good'? (Use the Train dataset)\n",
    "\n",
    "I'm going to start the exercise by filtering the rows in the \"score_string\" column that contain the word \"Perfect\". As you can see there is no record and I check this with the \"distinct\" command. No entry appears as \"Perfect\".\n",
    "\n",
    "Then I apply the same code for the records in which the word \"Good\" appears and I end up doing the same for \"Normal\" and \"Good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Puntuación &#39;Perfect&#39;\n",
       "0\n",
       "+------------+\n",
       "score_string|\n",
       "+------------+\n",
       "   Excellent|\n",
       "        Good|\n",
       "      Normal|\n",
       "+------------+\n",
       "\n",
       "\n",
       " Puntuación &#39;Good&#39;\n",
       "285831\n",
       "\n",
       " Puntuación &#39;Good&#39;+&#39;Normal&#39;\n",
       "289583\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Puntuación 'Perfect'\")\n",
    "print(df_spark_sql_train.filter('score_string=\"Perfect\"').count()) #no hay ninguno\n",
    "df_spark_sql_train.select('score_string').distinct().show() #lo compruebo con el comando \"distinct\" y veo que efectivamente no hay ninguna valoración \"Perfect\"\n",
    "print(\"\\n\",\"Puntuación 'Good'\")\n",
    "print(df_spark_sql_train.filter('score_string=\"Good\"').count())\n",
    "print(\"\\n\",\"Puntuación 'Good'+'Normal'\")\n",
    "print(df_spark_sql_train.filter('(score_string = \"Normal\") or (score_string = \"Good\")').count())\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Obtain the hotels with the highest average score, discarding all those with a score above Good. (Use the Train dataset) ###\n",
    "\n",
    "To do this exercise I started by removing the scores that are better than \"Good\" (\"Perfect\" and \"Excellent\").\n",
    "Then I have selected the columns that I wanted to display ('Hotel_Name', \"Average_Score\").\n",
    "Then I have grouped the data by \"Hotel_Name\" and calculated the average of the \"Average_Score\".\n",
    "I have continued with the command \"orderBy\" to sort the hotels in descending order according to their means (to get the hotels with the highest score).\n",
    "Finally, I have limited the results to 10 to get the 10 hotels with the highest average score, not including the hotels with a score above Good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+------------------+\n",
       "          Hotel_Name|avg(Average_Score)|\n",
       "+--------------------+------------------+\n",
       "Great Northern Ho...|  8.90000000000006|\n",
       "The Marylebone Hotel| 8.900000000000059|\n",
       "     Starhotels Echo| 8.900000000000059|\n",
       "Negresco Princess...| 8.900000000000057|\n",
       "Majestic Hotel Sp...| 8.900000000000052|\n",
       "Hotel Okura Amste...|  8.90000000000005|\n",
       "   K K Hotel Picasso| 8.900000000000048|\n",
       "Col n Hotel Barce...| 8.900000000000045|\n",
       "St Ermin s Hotel ...| 8.900000000000045|\n",
       "Hotel Saint Peter...| 8.900000000000043|\n",
       "+--------------------+------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_sql_train.filter('(score_string <> \"Perfect\") and (score_string <> \"Excellent\")').select('Hotel_Name',\"Average_Score\").groupBy('Hotel_Name').avg(\"Average_Score\").orderBy(\"avg(Average_Score)\",ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Apache Spark: Spark MLLib and Spark ML #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification: Decision Trees ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1: Look again at all the columns of the dataframe, to identify those that are categorical.###\n",
    "I apply the same code that I used in the first exercise but now with the dataframe \"df_spark_sql_train\" and I see that there are indeed categorical variables.\n",
    "To carry out the Machine learning processes, we must eliminate them or transform them into numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(&#39;Hotel_Address&#39;, &#39;string&#39;)\n",
       "(&#39;Additional_Number_of_Scoring&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Date&#39;, &#39;string&#39;)\n",
       "(&#39;Average_Score&#39;, &#39;double&#39;)\n",
       "(&#39;Hotel_Name&#39;, &#39;string&#39;)\n",
       "(&#39;Reviewer_Nationality&#39;, &#39;string&#39;)\n",
       "(&#39;Negative_Review&#39;, &#39;string&#39;)\n",
       "(&#39;Review_Total_Negative_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews&#39;, &#39;int&#39;)\n",
       "(&#39;Positive_Review&#39;, &#39;string&#39;)\n",
       "(&#39;Review_Total_Positive_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews_Reviewer_Has_Given&#39;, &#39;int&#39;)\n",
       "(&#39;Reviewer_Score&#39;, &#39;double&#39;)\n",
       "(&#39;Tags&#39;, &#39;string&#39;)\n",
       "(&#39;days_since_review&#39;, &#39;int&#39;)\n",
       "(&#39;lat&#39;, &#39;float&#39;)\n",
       "(&#39;lng&#39;, &#39;float&#39;)\n",
       "(&#39;score_string&#39;, &#39;string&#39;)\n",
       "(&#39;score_evaluation&#39;, &#39;int&#39;)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in df_spark_sql_train.dtypes:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Delete the variables 'Hotel_Address', 'Hotel_Name', 'Tags', 'Positive Review', 'Negative_Review' and 'score_string' from the df_spark_sql_train and df_spark_sql test dataframes. Call them: df_DT_train and df_DT_test.### \n",
    "\n",
    "I choose to eliminate the categorical variables, however I will transform \"Review_Date\" and \"Review_Nationality\" into numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(&#39;Additional_Number_of_Scoring&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Date&#39;, &#39;string&#39;)\n",
       "(&#39;Average_Score&#39;, &#39;double&#39;)\n",
       "(&#39;Reviewer_Nationality&#39;, &#39;string&#39;)\n",
       "(&#39;Review_Total_Negative_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Total_Positive_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews_Reviewer_Has_Given&#39;, &#39;int&#39;)\n",
       "(&#39;Reviewer_Score&#39;, &#39;double&#39;)\n",
       "(&#39;days_since_review&#39;, &#39;int&#39;)\n",
       "(&#39;lat&#39;, &#39;float&#39;)\n",
       "(&#39;lng&#39;, &#39;float&#39;)\n",
       "(&#39;score_evaluation&#39;, &#39;int&#39;)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_DT_train = df_spark_sql_train.drop(\"Hotel_Address\").drop(\"Hotel_Name\")\\\n",
    "  .drop(\"Tags\").drop(\"Positive_Review\").drop(\"Negative_Review\").drop(\"score_string\")\n",
    "df_DT_test = df_spark_sql_test.drop(\"Hotel_Address\").drop(\"Hotel_Name\")\\\n",
    "  .drop(\"Tags\").drop(\"Positive_Review\").drop(\"Negative_Review\").drop(\"score_string\")\n",
    "\n",
    "for i in df_DT_train.dtypes:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: For each remaining column that is String ('Review_Date' and 'Review_Nationality'), apply a StringIndexer (), returning the same column as a result, but with its name ending in _index. Overwrite both dataframes. ###\n",
    "\n",
    "In this exercise I convert the strings into numbers. I achieve this by passing the columns that have characters with the \"StringIndexer\" function, indicating to that function the column that I want to convert and the name of the resulting column.\n",
    "With the \"fit\" I choose the dataframe in which the column I want to transform will be searched to make the relevant calculations. With the \"transform\" I confirm that I want to transform the data, thus generating the \"output\" column.\n",
    "\n",
    "I apply this procedure for the train and test columns for the two variables mentioned in this exercise ('Review_Date' and 'Review_Nationality')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(&#39;Additional_Number_of_Scoring&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Date&#39;, &#39;string&#39;)\n",
       "(&#39;Average_Score&#39;, &#39;double&#39;)\n",
       "(&#39;Reviewer_Nationality&#39;, &#39;string&#39;)\n",
       "(&#39;Review_Total_Negative_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Total_Positive_Word_Counts&#39;, &#39;int&#39;)\n",
       "(&#39;Total_Number_of_Reviews_Reviewer_Has_Given&#39;, &#39;int&#39;)\n",
       "(&#39;Reviewer_Score&#39;, &#39;double&#39;)\n",
       "(&#39;days_since_review&#39;, &#39;int&#39;)\n",
       "(&#39;lat&#39;, &#39;float&#39;)\n",
       "(&#39;lng&#39;, &#39;float&#39;)\n",
       "(&#39;score_evaluation&#39;, &#39;int&#39;)\n",
       "(&#39;Review_Date_index&#39;, &#39;double&#39;)\n",
       "(&#39;Reviewer_Nationality_index&#39;, &#39;double&#39;)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df_DT_train = StringIndexer(inputCol=\"Review_Date\", outputCol=\"Review_Date_index\").fit(df_DT_train).transform(df_DT_train)\n",
    "df_DT_train = StringIndexer(inputCol='Reviewer_Nationality', outputCol=\"Reviewer_Nationality_index\").fit(df_DT_train).transform(df_DT_train)\n",
    "df_DT_test = StringIndexer(inputCol=\"Review_Date\", outputCol=\"Review_Date_index\").fit(df_DT_test).transform(df_DT_test)\n",
    "df_DT_test = StringIndexer(inputCol='Reviewer_Nationality', outputCol=\"Reviewer_Nationality_index\").fit(df_DT_test).transform(df_DT_test)\n",
    "\n",
    "\n",
    "for i in df_DT_train.dtypes:\n",
    "  print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Apply VectorAssembler () on the columns that are neither the previous two nor the 'score_evaluation' column, returning a column called 'features'. Call the result \"DT_vector_assembler\". ###\n",
    "\n",
    "In this exercise I create the Vector Assembler that will deposit all the information that exists in the dataframe in a single vector.\n",
    "This step is necessary as it is the way Spark ML expects to get the information.\n",
    "\n",
    "Since we have created the variables \"Review_Date_index\" and \"Reviewer_Nationality_index\" we indicate (using the drop command) that the vector does not include the variables\n",
    "\"Review_Date\" and \"Reviewer_Nationality\".\n",
    "Also, since \"score_evaluation\" is the column that we are going to predict, we don't include it either.\n",
    "\n",
    "Finally, I indicate that I want the output (the vector to be created) to be named \"features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "LR_vector_assembler = VectorAssembler(\\\n",
    "  inputCols=df_DT_train.drop(\"Review_Date\").drop(\"Reviewer_Nationality\").drop(\"score_evaluation\").columns,\\\n",
    "  outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Apply the transformer on both dataframes.###\n",
    "Now that the \"VectorAssembler\" has been created, I apply it with the \"transform\" and I give to it the data that must be transformed into vector, in this case the df_DT_train and the df_DT_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_DT_train = LR_vector_assembler.transform(df_DT_train)\n",
    "df_DT_test = LR_vector_assembler.transform(df_DT_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Initialize the decision tree model, train it and apply it to the test data. ###\n",
    "* Modelo: DecisionTreeClassifier:\n",
    "  * Label: score_evaluation.\n",
    "  * Features: features.\n",
    "  * maxBins: 1000\n",
    "  * maxDepth: 1\n",
    "  \n",
    "Since SparkMLLib requires that I have created the RDDs and in this case I have not, I am going to use the Spark ML libraries.\n",
    "\n",
    "I'm going to start by importing the necessary libraries.\n",
    "\n",
    "Next I am going to create the model specifying the \"label\", the name of the column with which I will train the model, as well as the depth of the model and the bins.\n",
    "Finally I train the model by passing the data from the training dataframe.\n",
    "\n",
    "I will finish the exercise 10 by applying the trained model to the data that I have saved for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">DataFrame[Additional_Number_of_Scoring: int, Review_Date: string, Average_Score: double, Reviewer_Nationality: string, Review_Total_Negative_Word_Counts: int, Total_Number_of_Reviews: int, Review_Total_Positive_Word_Counts: int, Total_Number_of_Reviews_Reviewer_Has_Given: int, Reviewer_Score: double, days_since_review: int, lat: float, lng: float, score_evaluation: int, Review_Date_index: double, Reviewer_Nationality_index: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(labelCol=\"score_evaluation\", featuresCol=\"features\",maxDepth=1, maxBins=1000).fit(df_DT_train)\n",
    "\n",
    "prediction = model.transform(df_DT_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: Evaluate the model applying a multiclass classifier. Calculate the 'accuracy' metric, and get the complementary to calculate the error. ###\n",
    "* Evaluador: MulticlassClassificationEvaluator\n",
    "  * Label: score_evaluation.\n",
    "  * Prediction: prediction.\n",
    "  * MetricName: accuracy.\n",
    "\n",
    "I'm going to start this exercise by importing the libraries that I need, then I create the evaluator where I indicate that I want to make a prediction with \"score_evaluation\" and that I am interested in the \"accuracy\" metric.\n",
    "\n",
    "Subsequently, the evaluator passed the variable \"prediction\" that I have obtained in the previous exercise to calculate the \"accuracy\".\n",
    "\n",
    "I finish this exercise by subtracting \"1-accuracy\" to find out the existing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">0.9887140640578633\n",
       " Error = 0.0112859 \n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"score_evaluation\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(accuracy)\n",
    "print(\" Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML: Pipelines ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines: Decision Trees###\n",
    "With the same concept as with KMeans, the flow for the decision trees will be designed. You must first apply the preprocessing changes seen above to the initial DataFrame to prepare it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: Remove the variables 'Hotel_Address', 'Hotel_Name', 'Tags', 'Positive Review', 'Negative_Review' and 'score_string' from the df_spark_sql_train and df_spark_sql test dataframes. Call them: df_DT_train and df_DT_test.### \n",
    "\n",
    "In this exercise I apply the same procedure as in exercise 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_DT_train = df_spark_sql_train.drop(\"Hotel_Address\").drop(\"Hotel_Name\")\\\n",
    "  .drop(\"Tags\").drop(\"Positive_Review\").drop(\"Negative_Review\").drop(\"score_string\")\n",
    "df_DT_test = df_spark_sql_test.drop(\"Hotel_Address\").drop(\"Hotel_Name\")\\\n",
    "  .drop(\"Tags\").drop(\"Positive_Review\").drop(\"Negative_Review\").drop(\"score_string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después se diseña el flujo para este modelo, el cual será:\n",
    "\n",
    "** StringIndexer --> VectorAssembler --> Decission Tree (Inicialización) --> Decission Tree (Entrenamiento) --> Modelo Decission Tree entrenado **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: Collect a list with all the StringIndexer to apply, and call it DT_string_indexers ###\n",
    "Instead of overwriting the dataframe each time, create a list, and with the 'append' method, all StringIndexers () will be added.\n",
    " \n",
    "I start with creating the Pipeline.\n",
    "In this case I indicate to the code that if the type of the column is equal to \"string\" then the code must create a variable \"StringIndexer\" that is saved in the variable \"DT_string_indexers\" to be able to correctly mount the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DT_string_indexers = []\n",
    "for dtype in df_DT_train.dtypes:\n",
    "  if dtype[1] == \"string\":\n",
    "    DT_string_indexers.append(StringIndexer(inputCol=dtype[0], outputCol=dtype[0]+\"_index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14: Save the application of the same VectorAssembler () of exercise 8 in the variable 'DT_vector_assembler'. ###\n",
    "I do the same as in the previous exercise, to mount the pipeline, I create a variable that contains the \"VectorAssembler\" saving the result in the variable \"D_vector_assembler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DT_vector_assembler = VectorAssembler(\\\n",
    "  inputCols=df_DT_train.drop(\"Review_Date\").drop(\"Reviewer_Nationality\").drop(\"score_evaluation\").columns,\\\n",
    "  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15: Create a list with the name DT_pipeline_stages, and add the list of StringIndexers and the VectorAssembler (in this order) ###\n",
    "\n",
    "In this exercise I simply deposit the information of the variables that I have created in the last two exercises in the \"DT_pipeline_stages\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DT_pipeline_stages = [str_indexer for str_indexer in DT_string_indexers]\n",
    "DT_pipeline_stages.append(DT_vector_assembler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 16: Initialize the decision tree model (same specifications as in ex. 10), and add it to the list of steps 'DT_pipeline_stages' ###\n",
    "\n",
    "The created model will also be inside the pipeline. This time, we did not train him (as we did in exercise 10), since we will do it in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(labelCol=\"score_evaluation\", featuresCol=\"features\",maxDepth=1, maxBins=1000)\n",
    "DT_pipeline_stages.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 17: Design the Pipeline and apply it to the Train data, calling it 'DT_pipeline_model' ###\n",
    "In this exercise we pass the entire pipeline created by the Pipeline function. This creates an object that we will apply in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Pipeline_1c30159ce883\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "DT_pipeline = Pipeline(stages=DT_pipeline_stages)\n",
    "print(DT_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 18: Apply the resulting model to the test data and evaluate it, as was done in exercice 11 ###\n",
    "\n",
    "Finally, I introduce the data to the pipeline, it will do all the steps that we have placed within it, ending with the process of \"training\" the model.\n",
    "\n",
    "I finish the exercise calculating the \"accuracy\" of the model with the dataframe data of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">0.9887140640578633\n",
       " Error = 0.0112859 \n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DT_pipeline_model = DT_pipeline.fit(df_DT_train)\n",
    "\n",
    "prediction = DT_pipeline_model.transform(df_DT_test)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"score_evaluation\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(accuracy)\n",
    "print(\" Error = %g \" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "AnaliticaEscalablePySparkEjercicios",
  "notebookId": 3510895540622380
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
