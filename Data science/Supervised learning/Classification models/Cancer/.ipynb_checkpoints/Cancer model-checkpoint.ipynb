{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, my name is Oriol-Boris Monjo Farr√©, \n",
    "I did a short code to create a classification model. \n",
    "I went trough different phases: \n",
    "\n",
    "    1-Importing & Cleaning Data\n",
    "    2-Feature Selection\n",
    "    3-Model Selection\n",
    "    4-Model Creation\n",
    "\n",
    "I will be explaining every step I make, I hope it can be useful.\n",
    "\n",
    "My main objective is to be able to predict with more than a 95% of accuracy \n",
    "whether a patient has cancer or not with the lowest number of variables possible.\n",
    "\n",
    "\n",
    "The original dataset has 11 variables and I would like to know if someone is ill with just 4 or less features.\n",
    "\n",
    "I start importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I import and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"breast-cancer-wisconsin.data.txt\",header=None)\n",
    "df.columns=[\"Sample code number\",\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now take a look to the different features available and I see that there is a ID column \n",
    "Since this will just confuse the classification model, this will be the fist variable to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "0             1000025                5                        1   \n",
      "1             1002945                5                        4   \n",
      "2             1015425                3                        1   \n",
      "3             1016277                6                        8   \n",
      "4             1017023                4                        1   \n",
      "\n",
      "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "0                         1                  1                            2   \n",
      "1                         4                  5                            7   \n",
      "2                         1                  1                            2   \n",
      "3                         8                  1                            3   \n",
      "4                         1                  3                            2   \n",
      "\n",
      "  Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
      "0           1                3                1        1      2  \n",
      "1          10                3                2        1      2  \n",
      "2           2                3                1        1      2  \n",
      "3           4                3                7        1      2  \n",
      "4           1                3                1        1      2  \n",
      "\n",
      "Index(['Clump Thickness', 'Uniformity of Cell Size',\n",
      "       'Uniformity of Cell Shape', 'Marginal Adhesion',\n",
      "       'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
      "       'Normal Nucleoli', 'Mitoses', 'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df=df.drop([\"Sample code number\"],axis=1)\n",
    "\n",
    "print(\"\")\n",
    "print(df.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a closer look, I realized there are interrogation signs in the column \"Bare Nuclei\", I assume that it means there is no data for these registers. \n",
    "There are several references with the interrogation sign, due to the high number of registersm, I preferer not to eliminate those lines. \n",
    "\n",
    "Instead I will substitute it for -99999 number. I do this because in a classification model, such a negative number will have a non significant impact on the result, so, it won't be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Bare Nuclei\"] == \"?\"] \n",
    "df=df.replace(\"?\", -99999)\n",
    "df[\"Bare Nuclei\"] = df[\"Bare Nuclei\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will divide the predictive features and the variable to predict. \n",
    "In the information document of this data frame it says that the \"Class\" variable (the one that I want to predict) has a 2 in the register where the person doesn't have cancer and 4 in the ones he/she has. \n",
    "\n",
    "Just because of a personal preference I will change all the 2 for 0 and all the 4 for 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"]=df[\"Class\"].replace(2,0)\n",
    "df[\"Class\"]=df[\"Class\"].replace(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I proceed assigning the column \"Class\" to the Y variable and all the other features to the X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df[\"Class\"]\n",
    "X=df.drop([\"Class\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                        Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start applying some techniques to reduce the number of variables. \n",
    "\n",
    "The fist one I will apply is the variance threshold analysis.\n",
    "This function will determine if there is any column that does not have a variance strong enough to be a good predictive variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                  --- VARIANCE TRESHOLD ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables 9\n",
      "List of varibles ['Clump Thickness' 'Uniformity of Cell Size' 'Uniformity of Cell Shape'\n",
      " 'Marginal Adhesion' 'Single Epithelial Cell Size' 'Bare Nuclei'\n",
      " 'Bland Chromatin' 'Normal Nucleoli' 'Mitoses']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_th = VarianceThreshold(threshold = 0.6)\n",
    "x_var = pd.DataFrame(var_th.fit_transform(X))\n",
    "print(\"Number of variables\", x_var.shape[1])\n",
    "print(\"List of varibles\", np.asarray(list(X))[var_th.get_support()])\n",
    "x_var.columns=['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape','Marginal Adhesion', 'Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses']\n",
    "\n",
    "X=x_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all the variables have a great level of variance, in fact, it is possible to confirm this fact by looking at the different features.\n",
    "\n",
    "Now I will proceed to apply the function SelectKBest with the function f_classif. \n",
    "These functions will apply to all the features the same test and select just 5, the ones that obtain the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                  --- SelectKBest --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables 5\n",
      "List of varibles ['Clump Thickness' 'Uniformity of Cell Size' 'Uniformity of Cell Shape'\n",
      " 'Bland Chromatin' 'Normal Nucleoli']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif \n",
    "\n",
    "\n",
    "var_sk = SelectKBest(f_classif, k = 5)\n",
    "x_sk = pd.DataFrame(var_sk.fit_transform(X, Y))  \n",
    "\n",
    "print(\"Number of variables\", x_sk.shape[1])\n",
    "\n",
    "print(\"List of varibles\", np.asarray(list(X))[var_sk.get_support()])\n",
    "\n",
    "x_sk.columns=np.asarray(list(X))[var_sk.get_support()]\n",
    "X=x_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last technique that I will apply, is the VIF analysis. \n",
    "This will reveal how correlated the predictive variables are.\n",
    " \n",
    "If a certain variable obtains a number greater than 5, it means that by creating a linear regression with the rest of variables it is possible to predict that feature. To create a model that contains repeated information can make it worst and because of that, if a feature obtains more than 5, I will delete the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                             --- VIF ANALYSIS --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " >>> VIF first try <<< \n",
      "\n",
      "     Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
      "VIF         1.824833                 6.482377                  6.229411   \n",
      "\n",
      "     Bland Chromatin  Normal Nucleoli  \n",
      "VIF         2.572217         2.337489  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculateVIF(data):\n",
    "    features = list(data.columns)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    result = pd.DataFrame(index = ['VIF'], columns = features)\n",
    "    result = result.fillna(0)\n",
    "    \n",
    "    for ite in range(num_features):\n",
    "        x_features = features[:]\n",
    "        y_featue = features[ite]\n",
    "        x_features.remove(y_featue)\n",
    "        \n",
    "        x = data[x_features]\n",
    "        y = data[y_featue]\n",
    "        \n",
    "        model.fit(data[x_features], data[y_featue])\n",
    "        \n",
    "        result[y_featue] = 1/(1 - model.score(data[x_features], data[y_featue]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\",20) \n",
    "print(\"\\n\" *3,\">>> VIF first try <<<\",\"\\n\")\n",
    "print(calculateVIF(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the \"Uniformity of Cell Size\" shows a high level of correlation \n",
    "with the other data, I will eliminate this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop([\"Uniformity of Cell Size\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will make this analysis again to see if the problem has ben solved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " >>> VIF second try <<< \n",
      "\n",
      "     Clump Thickness  Uniformity of Cell Shape  Bland Chromatin  \\\n",
      "VIF         1.802849                  3.170464         2.416836   \n",
      "\n",
      "     Normal Nucleoli  \n",
      "VIF         2.279781  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" *3,\">>> VIF second try <<<\",\"\\n\")\n",
    "print(calculateVIF(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables have obtained results under 5, so we can continue. \n",
    "\n",
    "Before proceeding to find a good model, I want to clarify that at this point I could scale of normalize the data, however, considering that all the data seem to be in the same scale, I will continue without transforming it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the right model I will use code that contain the following parts:\n",
    "    1-A dictionary with the different models and the parameters that I use the most for these models.\n",
    "\n",
    "    2-A for loop that contains the function \"GridSearchCV\" with the Cross-validation parameter set to 5.\n",
    "        That means that the data will be divided in 5 parts, using one part for testing and the other 4\n",
    "        for training the model. This procedure will be repeated 5 times and each time one different part \n",
    "        will be the one used for testing. \n",
    "\n",
    "        Finally, the score for every model will be displayed with the parameters that the model had \n",
    "        when it obtained the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    'svm': { \n",
    "        'model': svm.SVC(),\n",
    "        'params' : { \n",
    "            'C': [1,10,20],            \n",
    "            'kernel': ['rbf','linear'], \n",
    "            'gamma' :['scale','auto']\n",
    "        }  \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators': [5,10,50,100,200],\n",
    "            'criterion':['gini','entropy']            \n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n",
    "        'params': {\n",
    "            'C': [1,5,10]\n",
    "        }\n",
    "    },\n",
    "    'KNeighbors' : {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [5,10,20],\n",
    "            'p': [1,2],\n",
    "            'weights':['uniform','distance']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 model  best_score  \\\n",
      "3           KNeighbors    0.961398   \n",
      "0                  svm    0.958530   \n",
      "1        random_forest    0.955673   \n",
      "2  logistic_regression    0.955673   \n",
      "\n",
      "                                         best_params  \n",
      "3  {'n_neighbors': 10, 'p': 2, 'weights': 'distan...  \n",
      "0        {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}  \n",
      "1          {'criterion': 'gini', 'n_estimators': 50}  \n",
      "2                                           {'C': 1}  \n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for model_name, mp in model_params.items(): \n",
    "    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)  \n",
    "    clf.fit(X, Y) \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': clf.best_score_,\n",
    "        'best_params': clf.best_params_ \n",
    "    })\n",
    "    \n",
    "Modelos = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "pd.set_option(\"display.max_columns\",20)\n",
    "print(Modelos.sort_values(by=\"best_score\",ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the code has shown the best possible models, I will proceed to create it. \n",
    "However, I should first highlight that if I apply the results that I obtained with the code without taking a closer look, the created model may be overfitted. \n",
    " \n",
    "In order to find if this is the case, instead of using cross-validation to directly find the best model, I will divide the data using the \"train_test_split\" and different metrics to find if the different models have overfitting or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I divide the data in training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train ,X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors\n",
    "    The results obtained from the code suggest that I should use the following parameters: \n",
    "    KNeighborsClassifier(n_neighbors= 10, p= 2, weights= \"distance\")\n",
    "\n",
    "However, after taking a look on the metrics I realized that the model was overfitted In order to improve the situation, I have increased the number of n_neighbors and I changed the weights to \"uniform\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- TRAIN KNeighbors -------\n",
      "[[307  12]\n",
      " [ 10 160]] <-Confusion matrix\n",
      "Precision:  0.9550102249488752\n",
      "Exactitud:  0.9302325581395349\n",
      "Exhaustividad:  0.9411764705882353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       319\n",
      "           1       0.93      0.94      0.94       170\n",
      "\n",
      "    accuracy                           0.96       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.96      0.96      0.96       489\n",
      "\n",
      "\n",
      " ------- TEST KNeighbors -------\n",
      "[[136   3]\n",
      " [  5  66]] <-Confusion matrix\n",
      "Precision:  0.9619047619047619\n",
      "Exactitud:  0.9565217391304348\n",
      "Exhaustividad:  0.9295774647887324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       139\n",
      "           1       0.96      0.93      0.94        71\n",
      "\n",
      "    accuracy                           0.96       210\n",
      "   macro avg       0.96      0.95      0.96       210\n",
      "weighted avg       0.96      0.96      0.96       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mod_Knn=KNeighborsClassifier(n_neighbors= 15, p= 2, weights= \"uniform\")\n",
    "Mod_Knn.fit(X_train, Y_train)\n",
    "\n",
    "print(\"\\n\",\"------- TRAIN KNeighbors -------\")\n",
    "Pred_train_Knn=Mod_Knn.predict(X_train)\n",
    "print(confusion_matrix(Y_train,Pred_train_Knn),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_train,Pred_train_Knn))\n",
    "print(\"Exactitud: \", precision_score(Y_train,Pred_train_Knn))\n",
    "print(\"Exhaustividad: \",recall_score(Y_train,Pred_train_Knn))\n",
    "print(classification_report(Y_train,Pred_train_Knn))\n",
    "\n",
    "\n",
    "print(\"\\n\",\"------- TEST KNeighbors -------\")\n",
    "Pred_test_Knn=Mod_Knn.predict(X_test)\n",
    "print(confusion_matrix(Y_test,Pred_test_Knn),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_test,Pred_test_Knn))\n",
    "print(\"Exactitud: \", precision_score(Y_test,Pred_test_Knn))\n",
    "print(\"Exhaustividad: \",recall_score(Y_test,Pred_test_Knn))\n",
    "print(classification_report(Y_test,Pred_test_Knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- TRAIN SVC -------\n",
      "[[305  14]\n",
      " [  9 161]] <-Confusion matrix\n",
      "Precision:  0.9529652351738241\n",
      "Exactitud:  0.92\n",
      "Exhaustividad:  0.9470588235294117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       319\n",
      "           1       0.92      0.95      0.93       170\n",
      "\n",
      "    accuracy                           0.95       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.95      0.95      0.95       489\n",
      "\n",
      "\n",
      " ------- TEST SVC -------\n",
      "[[136   3]\n",
      " [  5  66]] <-Confusion matrix\n",
      "Precision:  0.9619047619047619\n",
      "Exactitud:  0.9565217391304348\n",
      "Exhaustividad:  0.9295774647887324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       139\n",
      "           1       0.96      0.93      0.94        71\n",
      "\n",
      "    accuracy                           0.96       210\n",
      "   macro avg       0.96      0.95      0.96       210\n",
      "weighted avg       0.96      0.96      0.96       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mod_SVC=svm.SVC(C= 1, gamma= 'scale', kernel= 'rbf')\n",
    "Mod_SVC.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(\"\\n\",\"------- TRAIN SVC -------\")\n",
    "Pred_train_SVC=Mod_SVC.predict(X_train)\n",
    "print(confusion_matrix(Y_train,Pred_train_SVC),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_train,Pred_train_SVC))\n",
    "print(\"Exactitud: \", precision_score(Y_train,Pred_train_SVC))\n",
    "print(\"Exhaustividad: \",recall_score(Y_train,Pred_train_SVC))\n",
    "print(classification_report(Y_train,Pred_train_SVC))\n",
    "\t\n",
    "\n",
    "print(\"\\n\",\"------- TEST SVC -------\")\n",
    "Pred_test_SVC=Mod_SVC.predict(X_test)\n",
    "print(confusion_matrix(Y_test,Pred_test_SVC),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_test,Pred_test_SVC))\n",
    "print(\"Exactitud: \", precision_score(Y_test,Pred_test_SVC))\n",
    "print(\"Exhaustividad: \",recall_score(Y_test,Pred_test_SVC))\n",
    "print(classification_report(Y_test,Pred_test_SVC))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "    The results obtained from the code suggest that I should use the following parameters: \n",
    "    RandomForestClassifier(criterion= 'gini', n_estimators= 5)\n",
    "\n",
    "    But the metrics showed that the model was overfitted. I solved the problem \n",
    "    by increasing the number of estimators, changing the max_features parameter to \"log2\" and,\n",
    "    the most important thing limiting the extension of the model setting the min_samples_split to 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- TRAIN Random Forest -------\n",
      "[[307  12]\n",
      " [  5 165]] <-Confusion matrix\n",
      "Precision:  0.9652351738241309\n",
      "Exactitud:  0.9322033898305084\n",
      "Exhaustividad:  0.9705882352941176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       319\n",
      "           1       0.93      0.97      0.95       170\n",
      "\n",
      "    accuracy                           0.97       489\n",
      "   macro avg       0.96      0.97      0.96       489\n",
      "weighted avg       0.97      0.97      0.97       489\n",
      "\n",
      "\n",
      " ------- TEST Random Forest -------\n",
      "[[136   3]\n",
      " [  3  68]] <-Confusion matrix\n",
      "Precision:  0.9714285714285714\n",
      "Exactitud:  0.9577464788732394\n",
      "Exhaustividad:  0.9577464788732394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       139\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       210\n",
      "   macro avg       0.97      0.97      0.97       210\n",
      "weighted avg       0.97      0.97      0.97       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mod_Ran=RandomForestClassifier(criterion= 'gini', max_features=\"log2\",n_estimators= 100, min_samples_split=40)\n",
    "Mod_Ran.fit(X_train, Y_train)\n",
    "\n",
    "print(\"\\n\",\"------- TRAIN Random Forest -------\")\n",
    "Pred_train_Ran=Mod_Ran.predict(X_train)\n",
    "print(confusion_matrix(Y_train,Pred_train_Ran),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_train,Pred_train_Ran))\n",
    "print(\"Exactitud: \", precision_score(Y_train,Pred_train_Ran))\n",
    "print(\"Exhaustividad: \",recall_score(Y_train,Pred_train_Ran))\n",
    "print(classification_report(Y_train,Pred_train_Ran))\n",
    "\n",
    "\n",
    "print(\"\\n\",\"------- TEST Random Forest -------\")\n",
    "Pred_test_Ran=Mod_Ran.predict(X_test)\n",
    "print(confusion_matrix(Y_test,Pred_test_Ran),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_test,Pred_test_Ran))\n",
    "print(\"Exactitud: \", precision_score(Y_test,Pred_test_Ran))\n",
    "print(\"Exhaustividad: \",recall_score(Y_test,Pred_test_Ran))\n",
    "print(classification_report(Y_test,Pred_test_Ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------- TRAIN logistic regession -------\n",
      "[[308  11]\n",
      " [ 11 159]] <-Confusion matrix\n",
      "Precision:  0.9550102249488752\n",
      "Exactitud:  0.9352941176470588\n",
      "Exhaustividad:  0.9352941176470588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       319\n",
      "           1       0.94      0.94      0.94       170\n",
      "\n",
      "    accuracy                           0.96       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.96      0.96      0.96       489\n",
      "\n",
      "\n",
      " ------- TEST logistic regession -------\n",
      "[[138   1]\n",
      " [  6  65]] <-Confusion matrix\n",
      "Precision:  0.9666666666666667\n",
      "Exactitud:  0.9848484848484849\n",
      "Exhaustividad:  0.9154929577464789\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       139\n",
      "           1       0.98      0.92      0.95        71\n",
      "\n",
      "    accuracy                           0.97       210\n",
      "   macro avg       0.97      0.95      0.96       210\n",
      "weighted avg       0.97      0.97      0.97       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mod_log=LogisticRegression(solver='liblinear',multi_class='auto', C=1)\n",
    "Mod_log.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(\"\\n\",\"------- TRAIN logistic regession -------\")\n",
    "Pred_train_log=Mod_log.predict(X_train)\n",
    "print(confusion_matrix(Y_train,Pred_train_log),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_train,Pred_train_log))\n",
    "print(\"Exactitud: \", precision_score(Y_train,Pred_train_log))\n",
    "print(\"Exhaustividad: \",recall_score(Y_train,Pred_train_log))\n",
    "print(classification_report(Y_train,Pred_train_log))\n",
    "\n",
    "\n",
    "print(\"\\n\",\"------- TEST logistic regession -------\")\n",
    "Pred_test_log=Mod_log.predict(X_test)\n",
    "print(confusion_matrix(Y_test,Pred_test_log),\"<-Confusion matrix\") \n",
    "print(\"Precision: \",accuracy_score(Y_test,Pred_test_log))\n",
    "print(\"Exactitud: \", precision_score(Y_test,Pred_test_log))\n",
    "print(\"Exhaustividad: \",recall_score(Y_test,Pred_test_log))\n",
    "print(classification_report(Y_test,Pred_test_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have solved the overfitting problems, that the KNeighbors and the Random Forest models showed, I will use the cross-validation function to definitely determine the score of the different models that I obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation KNeighbors: 0.9571017471736896\n",
      "Cross Validation SVC: 0.958530318602261\n",
      "Cross Validation Random Forest: 0.9599588900308325\n",
      "Cross Validation logistic regession: 0.9556731757451182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Mod_Knn=KNeighborsClassifier(n_neighbors= 15, p= 2, weights= \"uniform\")\n",
    "Mod_SVC=svm.SVC(C= 1, gamma= 'scale', kernel= 'rbf')\n",
    "Mod_Ran=RandomForestClassifier(criterion= 'gini', max_features=\"log2\",n_estimators= 10, min_samples_split=20)\n",
    "Mod_log=LogisticRegression(solver='liblinear',multi_class='auto', C=1)\n",
    "\n",
    "print(\"Cross Validation KNeighbors:\",cross_val_score(Mod_Knn, X, Y,cv=5).mean())\n",
    "print(\"Cross Validation SVC:\",cross_val_score(Mod_SVC, X, Y,cv=5).mean())\n",
    "print(\"Cross Validation Random Forest:\",cross_val_score(Mod_Ran, X, Y,cv=5).mean())\n",
    "print(\"Cross Validation logistic regession:\",cross_val_score(Mod_log, X, Y,cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: \n",
    "    -All the models that I have obtained show a score higher than 95% using just\n",
    "    -4 features out of the 11 variables that we have started with. \n",
    "    -The features that we can use to predict cancer are the following ones: \n",
    "    -\"Clump Thickness\", \"Uniformity of Cell Shape\", \"Bland Chromatin\" and \"Normal Nucleoli\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
